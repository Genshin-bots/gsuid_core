import re
import sys
import random
import asyncio
import logging
from typing import Any, Dict, Optional
from concurrent.futures import ThreadPoolExecutor

from joblib import dump, load
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer

from gsuid_core.logger import logger
from gsuid_core.data_store import get_res_path

# å®Œå…¨ç¦ç”¨ jieba çš„æ‰€æœ‰æ—¥å¿—è¾“å‡º
jieba_logger = logging.getLogger("jieba")
jieba_logger.setLevel(logging.CRITICAL)  # åªæ˜¾ç¤º CRITICAL çº§åˆ«çš„æ—¥å¿—
jieba_logger.propagate = False

# åŒæ—¶é‡å®šå‘ stdout/stderr æ¥æ•è· jieba çš„ç›´æ¥è¾“å‡º
_old_stdout = sys.stdout
_old_stderr = sys.stderr


class DevNull:
    def write(self, msg):
        pass

    def flush(self):
        pass


# ä¸´æ—¶é‡å®šå‘åˆ°ç©ºè®¾å¤‡
sys.stdout = DevNull()
sys.stderr = DevNull()

import jieba  # noqa: E402
import jieba.posseg as pseg  # noqa: E402

# æ¢å¤ stdout/stderr
sys.stdout = _old_stdout
sys.stderr = _old_stderr

AI_PATH = get_res_path("ai_core")
MODEL_PATH = AI_PATH / "intent_classifier.joblib"


ACTION_VERBS = {
    "æŸ¥",
    "çœ‹",
    "æ‰¾",
    "æœ",
    "æŸ¥è¯¢",
    "æœç´¢",
    "åˆ†æ",
    "ç”Ÿæˆ",
    "æ‰“å¼€",
    "è®¡ç®—",
    "æ¨è",
    "ç¿»è¯‘",
    "è§£é‡Š",
    "å†™",
    "åš",
    "ç”»",
    "æ¥",
    "æŸ¥æŸ¥",
    "çœ‹çœ‹",
    "æœæœ",
    "æµ‹",
    "ä¼°ç®—",
    "ç›‘æ§",
    "æ˜¾ç¤º",
    "åˆ—ä¸¾",
}

FUNCTIONAL_NOUNS = {
    "é¢æ¿",
    "æ•°æ®",
    "å±æ€§",
    "æ’è¡Œ",
    "æ’è¡Œæ¦œ",
    "æ¦œå•",
    "æ”»ç•¥",
    "è®°å½•",
    "æˆ˜ç»©",
    "è‚¡ä»·",
    "èµ°åŠ¿",
    "è¡Œæƒ…",
    "ä»·æ ¼",
    "æ±‡ç‡",
    "å¤§ç›˜",
    "é‡‘ä»·",
    "æ²¹ä»·",
    "æ°”æ¸©",
    "å¤©æ°”",
    "é…ç½®",
    "è£…å¤‡",
    "åœ£é—ç‰©",
    "è¯„åˆ†",
    "ç»ƒåº¦",
    "è¯¦æƒ…",
    "ä¿¡æ¯",
    "æƒ…å†µ",
    "çŠ¶æ€",
    "æ•°å€¼",
    "å€ç‡",
    "æ¦‚ç‡",
    "æ‰è½",
    "æˆæœ¬",
    "æ”¶ç›Š",
}

NEGATION_WORDS = {"ä¸", "æ²¡", "æ— ", "é", "è«", "åˆ«", "ä¸è¦", "ä¸ç”¨", "ä¼‘æƒ³", "ç¦æ­¢", "åˆ«å»", "ä¼‘"}

STATE_WORDS = {
    "éº»",
    "éº»äº†",
    "äº",
    "äºæ­»",
    "æ•‘å‘½",
    "å§æ§½",
    "ç‰›é€¼",
    "ç¬‘æ­»",
    "æ— è¯­",
    "666",
    "ä¸‘",
    "å¤ªä¸‘",
    "çœŸä¸‘",
    "éš¾çœ‹",
    "åƒåœ¾",
    "å‘",
    "è¯ä¸¸",
    "å´©",
    "å´©äº†",
    "æ°´",
    "éš¾",
    "å¥½éš¾",
    "å¤ªéš¾",
    "ä¸è¡Œ",
    "ä¸€èˆ¬",
    "å·®",
    "å¼º",
    "å¼±",
    "ç¦»è°±",
    "æ¶å¿ƒ",
    "å¡",
    "æ…¢",
    "è´µ",
    "ä¾¿å®œ",
    "å¥½",
    "å",
    "é«˜",
    "ä½",
    "çƒ‚",
    "æ‹‰èƒ¯",
    "æ€ª",
    "å¯„",
    "æ™¦æ°”",
    "è°¢",
    "è°¢äº†",
    " thanks",
    "ok",
    "æ‡‚",
    "æ˜ç™½",
    "ç†è§£",
    "æ¸…æ¥š",
    "çŸ¥é“",
    "è¿·ç³Š",
    "æ™•",
    "æ‡µ",
    "ç–‘æƒ‘",
}

QUERY_WORDS = {"æ€ä¹ˆ", "å¤šå°‘", "ä»€ä¹ˆ", "è°", "å“ªé‡Œ", "å‡ ", "å—", "å‘¢", "å•¥", "å’‹", "å¦‚ä½•", "ä¸ºä»€ä¹ˆ"}


# åˆå§‹åŒ– Jieba
def init_jieba():
    for w in FUNCTIONAL_NOUNS:
        jieba.add_word(w, tag="n_prop")
    for w in NEGATION_WORDS:
        jieba.add_word(w, tag="d_neg")
    for w in STATE_WORDS:
        jieba.add_word(w, tag="a_state")
    for w in ACTION_VERBS:
        jieba.add_word(w, tag="v_act")
    for w in QUERY_WORDS:
        jieba.add_word(w, tag="r_query")


init_jieba()


class ItemSelector(BaseEstimator, TransformerMixin):
    """ç”¨äºåœ¨ Pipeline ä¸­é€‰æ‹©å­—å…¸æ•°æ®çš„ç‰¹å®š Key"""

    def __init__(self, key):
        self.key = key

    def fit(self, x, y=None):
        return self

    def transform(self, data_dict):
        return data_dict[self.key]


def smart_abstraction(text: str) -> str:
    """
    é€»è¾‘ï¼šå°†æ–‡æœ¬è½¬åŒ–ä¸ºæŠ½è±¡æ ‡ç­¾åºåˆ—ï¼Œä¾‹å¦‚ "æŸ¥é›·ç¥é¢æ¿" -> "<ACT> <ENT> <PROP>"
    """
    words = pseg.cut(text)
    clean_tokens = []

    for word, flag in words:
        w = word.lower()
        if flag == "d_neg" or w in NEGATION_WORDS:
            clean_tokens.append("<NEG>")
        elif flag == "n_prop" or w in FUNCTIONAL_NOUNS:
            clean_tokens.append("<PROP>")
        elif flag == "a_state" or w in STATE_WORDS:
            clean_tokens.append("<STATE>")
        elif flag == "v_act" or w in ACTION_VERBS:
            clean_tokens.append("<ACT>")
        elif flag == "r_query" or w in QUERY_WORDS or "?" in w or "ï¼Ÿ" in w:
            clean_tokens.append("<QUERY>")
        else:
            if flag.startswith("n") or flag.startswith("v") or flag.startswith("x"):
                clean_tokens.append("<ENT>")
            elif w.strip():
                clean_tokens.append(w)

    return " ".join(clean_tokens)


class IntentService:
    def __init__(self, model_path=MODEL_PATH, num_threads=4):
        self.model_path = model_path
        self.executor = ThreadPoolExecutor(max_workers=num_threads)
        self.model = None
        self._load_or_train()

    def _load_or_train(self):
        """å°è¯•åŠ è½½æ¨¡å‹ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥åˆ™å¼ºåˆ¶é‡æ–°è®­ç»ƒ"""
        # æ ‡è®°æ˜¯å¦éœ€è¦è®­ç»ƒ
        need_train = False

        if self.model_path.exists():
            try:
                # å°è¯•è¯»å–ç°æœ‰æ¨¡å‹
                self.model = load(self.model_path)
                logger.debug(f"[Info] æ¨¡å‹å·²åŠ è½½: {self.model_path}")
            except Exception as e:
                logger.warning(f"[Error] æ¨¡å‹åŠ è½½å¤±è´¥ (ç‰ˆæœ¬ä¸å…¼å®¹æˆ–è·¯å¾„é”™è¯¯): {e}")
                logger.warning("[Info] æ­£åœ¨é‡æ–°è®­ç»ƒæ¨¡å‹ä»¥ä¿®å¤æ­¤é—®é¢˜...")
                need_train = True
        else:
            logger.debug(f"[Warning] æ¨¡å‹æ–‡ä»¶ {self.model_path} ä¸å­˜åœ¨ã€‚")
            need_train = True

        # å¦‚æœéœ€è¦è®­ç»ƒï¼ˆæ–‡ä»¶ä¸å­˜åœ¨ æˆ– åŠ è½½æŠ¥é”™ï¼‰
        if need_train:
            self.train()

    def _generate_enhanced_data(self):
        tool_samples = []
        chat_samples = []
        entities = ["é›·ç¥", "èŒ…å°", "çº³æŒ‡", "ç‹è€…è£è€€", "åŸç¥", "è¿™åªè‚¡ç¥¨", "ä»Šå¤©", "Aè‚¡"]

        tool_patterns = [
            "<ACT> <ENT>",
            "<ACT> <PROP>",
            "<ACT> <ENT> <PROP>",
            "<ENT> <PROP>",
            "<ENT> çš„ <PROP>",
            "<ENT> <ACT> <PROP>",
            "<ACT> <ENT> <PROP> <QUERY>",
        ]

        chat_patterns = [
            "<NEG> <ACT>",
            "<NEG> <ACT> <ENT>",
            "<PROP> <STATE>",
            "<PROP> <NEG> <STATE>",
            "<ENT> <STATE>",
            "<ENT> <NEG> <STATE>",
            "<STATE>",
            "<ENT> <ACT> <STATE>",
            "<ACT> <NEG> <ACT>",
            "æˆ‘ <NEG> çŸ¥é“",
            "<ACT> <NEG> <STATE>",
            "<ACT> <NEG> <ENT>",
            "<ENT> <QUERY>",
        ]

        # ç”Ÿæˆå·¥å…·æ•°æ®
        for pattern in tool_patterns:
            for ent in entities:
                text = pattern.replace("<ENT>", ent)
                if "<ACT>" in text:
                    text = text.replace("<ACT>", random.choice(list(ACTION_VERBS)))
                if "<PROP>" in text:
                    text = text.replace("<PROP>", random.choice(list(FUNCTIONAL_NOUNS)))
                if "<QUERY>" in text:
                    text = text.replace("<QUERY>", random.choice(list(QUERY_WORDS)))
                tool_samples.append(text)

        # ç”Ÿæˆé—²èŠæ•°æ®
        for pattern in chat_patterns:
            for ent in entities:
                text = pattern.replace("<ENT>", ent)
                if "<ACT>" in text:
                    text = text.replace("<ACT>", random.choice(list(ACTION_VERBS)))
                if "<PROP>" in text:
                    text = text.replace("<PROP>", random.choice(list(FUNCTIONAL_NOUNS)))
                if "<STATE>" in text:
                    text = text.replace("<STATE>", random.choice(list(STATE_WORDS)))
                if "<NEG>" in text:
                    text = text.replace("<NEG>", random.choice(list(NEGATION_WORDS)))
                chat_samples.append(text)

        extra_chats = [
            "è¿™æ•°æ®å¤ªçœŸå®äº†",
            "å±æ€§æ‹‰èƒ¯",
            "çœ‹ä¸æ‡‚è¿™ä¸ªèµ°åŠ¿",
            "è¿™æ˜¯ä»€ä¹ˆé¬¼æ”»ç•¥",
            "åˆ«ç»™æˆ‘çœ‹è¿™äº›",
            "ä¸è¦åˆ†æ",
            "æˆ‘ä¸æŸ¥",
            "ç®—äº†å§",
        ]
        chat_samples.extend(extra_chats * 5)

        min_len = min(len(tool_samples), len(chat_samples))
        X = tool_samples[:min_len] + chat_samples[:min_len]
        y = ["å·¥å…·"] * min_len + ["é—²èŠ"] * min_len
        return X, y

    def train(self):
        """è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹"""
        logger.debug("[Info] å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        X_raw, y = self._generate_enhanced_data()
        X_abstract = [smart_abstraction(text) for text in X_raw]
        X_train_dict = {"raw": X_raw, "abs": X_abstract}

        pipeline = Pipeline(
            [
                (
                    "union",
                    FeatureUnion(
                        transformer_list=[
                            (
                                "abs_features",
                                Pipeline(
                                    [
                                        ("selector", ItemSelector(key="abs")),
                                        (
                                            "tfidf",
                                            TfidfVectorizer(token_pattern=r"(?u)\b\w+\b|<\w+>", ngram_range=(1, 3)),
                                        ),
                                    ]
                                ),
                            ),
                            (
                                "raw_features",
                                Pipeline(
                                    [
                                        ("selector", ItemSelector(key="raw")),
                                        (
                                            "tfidf",
                                            TfidfVectorizer(analyzer="char_wb", ngram_range=(2, 4), max_features=5000),
                                        ),
                                    ]
                                ),
                            ),
                        ]
                    ),
                ),
                ("clf", LogisticRegression(C=1.0, solver="liblinear", class_weight="balanced")),
            ]
        )

        pipeline.fit(X_train_dict, y)
        dump(pipeline, self.model_path)
        self.model = pipeline
        logger.debug(f"[Info] æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶ä¿å­˜è‡³: {self.model_path}")

    def _rule_based_check(self, text: str) -> Optional[Dict[str, Any]]:
        """ä¼˜å…ˆæ‰§è¡Œçš„æ­£åˆ™/é€»è¾‘è§„åˆ™"""

        # è§„åˆ™ 0: è¯¢é—®å¤§æ¨¡å‹è‡ªèº«çš„é—®é¢˜ = é—²èŠ (ä½ æ˜¯ä»€ä¹ˆ/ä½ æ˜¯è°/ä½ ä½¿ç”¨ä»€ä¹ˆæ¨¡å‹)
        if re.search(
            r"^(æˆ‘|ä½ ).*(æ˜¯|ä½¿ç”¨|èƒ½|ä¼š).*(ä»€ä¹ˆ|è°|å•¥|æ€ä¹ˆ|å¤šå°‘|å¤šå¤§|åå­—|å‹å·).*(æ¨¡å‹|AI|åŠ©æ‰‹|æœºå™¨äºº|ç‰ˆæœ¬)", text
        ):
            return {"intent": "é—²èŠ", "conf": 0.99, "reason": "Rule: SelfReference"}

        # è§„åˆ™ 1: ä»£è¯+ç–‘é—® = é—²èŠ (è¿™æ˜¯ä»€ä¹ˆ/é‚£æ˜¯è°)
        if re.search(r"^(è¿™|é‚£|æˆ‘|ä½ |ä»–|å¥¹|å®ƒ|å“ª|è°).*(ä»€ä¹ˆ|å’‹|è°|å“ª|å—|å‘¢)[?ï¼Ÿ]?$", text):
            return {"intent": "é—²èŠ", "conf": 0.98, "reason": "Rule: Pronoun+Query"}

        # è§„åˆ™ 2: çº¯ç–‘é—®/æƒ…ç»ªè¡¨è¾¾ = é—²èŠ (ä¸ºä»€ä¹ˆ/å’‹å›äº‹/å•Šå•Šå•Š)
        if re.search(r"^(ä¸ºä»€ä¹ˆ|å’‹å›äº‹|å•Š|å“å‘€|å‘œå‘œ|å“¼|å‘µå‘µ|å“ˆå“ˆ|å“‡|å”‰|å“å“Ÿ)+[!?ğŸ˜­ğŸ˜­ğŸ˜¢ğŸ˜±ğŸ˜¡ğŸ™]+.*$", text):
            return {"intent": "é—²èŠ", "conf": 0.95, "reason": "Rule: PureEmotion"}

        # è§„åˆ™ 3: è¯¢é—®è§‚ç‚¹/èº«ä»½/æ¨¡æ‹Ÿ/å»ºè®® = é—²èŠ (ä½ å¯¹...çœ‹æ³•/æ¨¡æ‹Ÿ.../ä½ åº”è¯¥...)
        if re.search(r".*(ä½ å¯¹.*çœ‹æ³•|ä½ è§‰å¾—|ä½ è®¤ä¸º|æ¨¡æ‹Ÿ|æ˜¯.*åŒ–èº«|ä½ åº”è¯¥|ä½ è¦|ä½ æ¯).*", text):
            return {"intent": "é—²èŠ", "conf": 0.93, "reason": "Rule: OpinionOrSimulate"}

        # è§„åˆ™ 4: åŠ¨è¯+å¦å®š/çŠ¶æ€ = é—²èŠ (çœ‹ä¸æ‡‚/åšä¸åˆ°)
        if re.search(r"(æŸ¥|çœ‹|æœ|æ‰¾|åˆ†æ|ç®—|å¬|è¯´)(ä¸|æ²¡|æ— æ³•|ä¸èƒ½)(æ‡‚|äº†|åˆ°|è¡Œ|å¥½|æ˜ç™½)", text):
            return {"intent": "é—²èŠ", "conf": 0.97, "reason": "Rule: Act+Neg+State"}

        # è§„åˆ™ 5: å¼ºå¦å®š + åŠ¨ä½œ = é—²èŠ (ä¸è¦æŸ¥)
        if re.search(r"[ä¸åˆ«æ²¡é][è¦]?.*?(æŸ¥|çœ‹|æœ|åˆ†æ|ç®—|æµ‹)", text):
            return {"intent": "é—²èŠ", "conf": 0.99, "reason": "Rule: Negation+Action"}

        # è§„åˆ™ 6: çº¯æƒ…ç»ª/çŠ¶æ€è¯ä¸»å¯¼
        has_state = any(s in text for s in STATE_WORDS)
        has_query = any(q in text for q in QUERY_WORDS)
        has_prop = any(p in text for p in FUNCTIONAL_NOUNS)

        # å¦‚æœåŒ…å«çŠ¶æ€è¯ï¼Œä¸”æ²¡æœ‰æ˜ç¡®çš„ç–‘é—®è¯
        if has_state and not has_query:
            if has_prop:
                return {"intent": "é—²èŠ", "conf": 0.95, "reason": "Rule: Prop+State"}

        return None

    def _sync_predict(self, text: str) -> Dict[str, Any]:
        rule_result = self._rule_based_check(text)
        if rule_result:
            return {"text": text, **rule_result}

        if self.model is None:
            return {"text": text, "intent": "Error", "conf": 0.0, "reason": "Model Not Loaded"}

        abstracted = smart_abstraction(text)
        input_data = {"raw": [text], "abs": [abstracted]}

        try:
            probs = self.model.predict_proba(input_data)[0]
            intent_idx = probs.argmax()
            intent = self.model.classes_[intent_idx]
            confidence = float(probs[intent_idx])
            return {"text": text, "intent": intent, "conf": round(confidence, 4), "reason": "Model"}
        except Exception as e:
            return {"text": text, "intent": "Error", "conf": 0.0, "reason": str(e)}

    async def predict_async(self, text: str) -> Dict[str, Any]:
        """å¤–éƒ¨è°ƒç”¨çš„å¼‚æ­¥æ¥å£"""
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(self.executor, self._sync_predict, text)


# ==========================================
# 4. æµ‹è¯•ä¸è¿è¡Œ
# ==========================================


async def benchmark(service: IntentService):
    test_cases = [
        "æŸ¥é¢æ¿",
        "ç«ç¥é¢æ¿æ€ä¹ˆæå‡",
        "å¸®æˆ‘çœ‹çœ‹æ·±æ¸Šè®°å½•",
        "æŸ¥ä¸€ä¸‹èŒ…å°è‚¡ä»·",
        "çœ‹çœ‹è‹±ä¼Ÿè¾¾èµ°åŠ¿",
        "æ‰“å¼€ç©ºè°ƒ",
        "å¸®æˆ‘å…³ç¯",
        "é¢æ¿å¤ªä¸‘äº†",
        "æ·±æ¸Šå¥½éš¾æ‰“",
        "æ•°æ®ä¸å¤ªå¥½",
        "èŒ…å°è·Œå¾—å¥½æƒ¨",
        "è‚¡ç¥¨äºéº»äº†",
        "å§æ§½æ€ä¹ˆå›äº‹",
        "è¿™æ˜¯ä»€ä¹ˆ",
        "çœ‹ä¸æ‡‚",
        "ä¸è¦æŸ¥",
        "èŒ…å°è·Œäº†å—",
        "å…‰çº¿ä¼ åª’æœ€è¿‘å…­ä¸ªæœˆæ¶¨çš„æ€ä¹ˆæ ·",
        "ä½ æ˜¯ä½¿ç”¨ä»€ä¹ˆæ¨¡å‹ï¼Ÿ",
        "ä¸ºä»€ä¹ˆğŸ˜­",
        "ä¸ºä»€ä¹ˆ!",
        "è¯·é—®ä½ å¯¹å°æ‹›å–µæ˜¯ä»€ä¹ˆçœ‹æ³•ï¼Ÿ",
        "ä½ å¯¹æŠ±æŠ±çš„çœ‹æ³•æ˜¯ï¼Ÿä½ æ˜¯ä¸€ä¸ªçŒ«å¨˜",
        "æ¨¡æ‹Ÿå°ç‹—çš„å«å£°",
        "ä½ æ—¢ç„¶æ˜¯å°æ‹›å–µçš„åŒ–èº«ï¼Œä½ æ²¡ä¸€å¥è¯çš„ç»“å°¾åº”è¯¥åŠ ä¸€ä¸ªâ€œå–µâ€å­—",
    ]

    logger.debug(f"{'Input':<20} | {'Intent':<10} | {'Conf':<5} | {'Reason'}")
    logger.debug("-" * 65)

    tasks = [service.predict_async(t) for t in test_cases]
    results = await asyncio.gather(*tasks)

    for res in results:
        logger.debug(f"{res['text']:<20} | {res['intent']:<10} | {res['conf']:<5} | {res.get('reason', '-')}")


classifier_service = IntentService(model_path=MODEL_PATH)

if __name__ == "__main__":
    asyncio.run(benchmark(classifier_service))
