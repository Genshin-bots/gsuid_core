import re
import sys
import random
import asyncio
import logging
from typing import Any, Dict, Optional
from concurrent.futures import ThreadPoolExecutor

from joblib import dump, load
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer

from gsuid_core.logger import logger
from gsuid_core.data_store import get_res_path

# å®Œå…¨ç¦ç”¨ jieba çš„æ‰€æœ‰æ—¥å¿—è¾“å‡º
jieba_logger = logging.getLogger("jieba")
jieba_logger.setLevel(logging.CRITICAL)
jieba_logger.propagate = False

_old_stdout = sys.stdout
_old_stderr = sys.stderr


class DevNull:
    def write(self, msg):
        pass

    def flush(self):
        pass


sys.stdout = DevNull()
sys.stderr = DevNull()

import jieba  # noqa: E402
import jieba.posseg as pseg  # noqa: E402

sys.stdout = _old_stdout
sys.stderr = _old_stderr

AI_PATH = get_res_path("ai_core")
MODEL_PATH = AI_PATH / "intent_classifier_v2.joblib"

# ==========================================
# 1. è¯å…¸å®šä¹‰ (æ–°å¢äº† KNOWLEDGE_NOUNS)
# ==========================================

ACTION_VERBS = {
    "æŸ¥",
    "çœ‹",
    "æ‰¾",
    "æœ",
    "æŸ¥è¯¢",
    "æœç´¢",
    "åˆ†æ",
    "ç”Ÿæˆ",
    "æ‰“å¼€",
    "è®¡ç®—",
    "æ¨è",
    "ç¿»è¯‘",
    "è§£é‡Š",
    "å†™",
    "åš",
    "ç”»",
    "æ¥",
    "æŸ¥æŸ¥",
    "çœ‹çœ‹",
    "æœæœ",
    "æµ‹",
    "ä¼°ç®—",
    "ç›‘æ§",
    "æ˜¾ç¤º",
    "åˆ—ä¸¾",
}

FUNCTIONAL_NOUNS = {
    "é¢æ¿",
    "æ•°æ®",
    "å±æ€§",
    "æ’è¡Œ",
    "æ’è¡Œæ¦œ",
    "æ¦œå•",
    "è®°å½•",
    "æˆ˜ç»©",
    "è‚¡ä»·",
    "èµ°åŠ¿",
    "è¡Œæƒ…",
    "ä»·æ ¼",
    "æ±‡ç‡",
    "å¤§ç›˜",
    "é‡‘ä»·",
    "æ²¹ä»·",
    "æ°”æ¸©",
    "å¤©æ°”",
    "é…ç½®",
    "è£…å¤‡",
    "åœ£é—ç‰©",
    "è¯„åˆ†",
    "ç»ƒåº¦",
    "è¯¦æƒ…",
    "ä¿¡æ¯",
    "æƒ…å†µ",
    "çŠ¶æ€",
    "æ•°å€¼",
    "å€ç‡",
    "æ¦‚ç‡",
    "æ‰è½",
    "æˆæœ¬",
    "æ”¶ç›Š",
}

# [æ–°å¢] ä¸“é—¨ç”¨äº RAG é—®ç­”çš„çŸ¥è¯†ç±»åè¯
KNOWLEDGE_NOUNS = {
    "è¡€é‡",
    "æœºåˆ¶",
    "å‰§æƒ…",
    "é…é˜Ÿ",
    "é˜Ÿä¼",
    "æ­¦å™¨",
    "èƒŒæ™¯",
    "æ•…äº‹",
    "ä»‹ç»",
    "å¼±ç‚¹",
    "ä½ç½®",
    "ææ–™",
    "é…æ–¹",
    "æ‰“æ³•",
    "å‡ºå¤„",
    "ä¸–ç•Œè§‚",
    "å¤©èµ‹",
    "å‘½åº§",
    "æŠ€èƒ½",
    "æˆå°±",
    "ä»»åŠ¡",
    "å½©è›‹",
    "è®¾å®š",
    "æ”»ç•¥",
}

NEGATION_WORDS = {"ä¸", "æ²¡", "æ— ", "é", "è«", "åˆ«", "ä¸è¦", "ä¸ç”¨", "ä¼‘æƒ³", "ç¦æ­¢", "åˆ«å»", "ä¼‘"}

STATE_WORDS = {
    "éº»",
    "éº»äº†",
    "äº",
    "äºæ­»",
    "æ•‘å‘½",
    "å§æ§½",
    "ç‰›é€¼",
    "ç¬‘æ­»",
    "æ— è¯­",
    "666",
    "ä¸‘",
    "å¤ªä¸‘",
    "çœŸä¸‘",
    "éš¾çœ‹",
    "åƒåœ¾",
    "å‘",
    "è¯ä¸¸",
    "å´©",
    "å´©äº†",
    "æ°´",
    "éš¾",
    "å¥½éš¾",
    "å¤ªéš¾",
    "ä¸è¡Œ",
    "ä¸€èˆ¬",
    "å·®",
    "å¼º",
    "å¼±",
    "ç¦»è°±",
    "æ¶å¿ƒ",
    "å¡",
    "æ…¢",
    "è´µ",
    "ä¾¿å®œ",
    "å¥½",
    "å",
    "é«˜",
    "ä½",
    "çƒ‚",
    "æ‹‰èƒ¯",
    "æ€ª",
    "å¯„",
    "æ™¦æ°”",
    "è°¢",
    "è°¢äº†",
    " thanks",
    "ok",
    "æ‡‚",
    "æ˜ç™½",
    "ç†è§£",
    "æ¸…æ¥š",
    "çŸ¥é“",
    "è¿·ç³Š",
    "æ™•",
    "æ‡µ",
    "ç–‘æƒ‘",
}

QUERY_WORDS = {"æ€ä¹ˆ", "å¤šå°‘", "ä»€ä¹ˆ", "è°", "å“ªé‡Œ", "å‡ ", "å—", "å‘¢", "å•¥", "å’‹", "å¦‚ä½•", "ä¸ºä»€ä¹ˆ"}


# åˆå§‹åŒ– Jieba
def init_jieba():
    for w in FUNCTIONAL_NOUNS:
        jieba.add_word(w, tag="n_prop")
    for w in KNOWLEDGE_NOUNS:  # [æ–°å¢] æ³¨å†ŒçŸ¥è¯†åè¯
        jieba.add_word(w, tag="n_know")
    for w in NEGATION_WORDS:
        jieba.add_word(w, tag="d_neg")
    for w in STATE_WORDS:
        jieba.add_word(w, tag="a_state")
    for w in ACTION_VERBS:
        jieba.add_word(w, tag="v_act")
    for w in QUERY_WORDS:
        jieba.add_word(w, tag="r_query")


init_jieba()


class ItemSelector(BaseEstimator, TransformerMixin):
    def __init__(self, key):
        self.key = key

    def fit(self, x, y=None):
        return self

    def transform(self, data_dict):
        return data_dict[self.key]


def smart_abstraction(text: str) -> str:
    words = pseg.cut(text)
    clean_tokens = []

    for word, flag in words:
        w = word.lower()
        if flag == "d_neg" or w in NEGATION_WORDS:
            clean_tokens.append("<NEG>")
        elif flag == "n_prop" or w in FUNCTIONAL_NOUNS:
            clean_tokens.append("<PROP>")
        elif flag == "n_know" or w in KNOWLEDGE_NOUNS:  # [æ–°å¢] æŠ½è±¡å‡º KNOW æ ‡ç­¾
            clean_tokens.append("<KNOW>")
        elif flag == "a_state" or w in STATE_WORDS:
            clean_tokens.append("<STATE>")
        elif flag == "v_act" or w in ACTION_VERBS:
            clean_tokens.append("<ACT>")
        elif flag == "r_query" or w in QUERY_WORDS or "?" in w or "ï¼Ÿ" in w:
            clean_tokens.append("<QUERY>")
        else:
            if flag.startswith("n") or flag.startswith("v") or flag.startswith("x"):
                clean_tokens.append("<ENT>")
            elif w.strip():
                clean_tokens.append(w)

    return " ".join(clean_tokens)


class IntentService:
    def __init__(self, model_path=MODEL_PATH, num_threads=4):
        self.model_path = model_path
        self.executor = ThreadPoolExecutor(max_workers=num_threads)
        self.model = None
        self._load_or_train()

    def _load_or_train(self):
        need_train = False
        if self.model_path.exists():
            try:
                self.model = load(self.model_path)
                # [æ–°å¢æ£€æŸ¥] å¦‚æœè¯»å–åˆ°çš„æ—§æ¨¡å‹åªæœ‰2ä¸ªåˆ†ç±»ï¼Œå¼ºåˆ¶é‡æ–°è®­ç»ƒ
                if len(self.model.classes_) < 3:
                    logger.warning("[Info] æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬æ¨¡å‹ (åˆ†ç±»ä¸è¶³3ä¸ª)ï¼Œå³å°†é‡æ–°è®­ç»ƒ...")
                    need_train = True
                else:
                    logger.debug(f"[Info] æ¨¡å‹å·²åŠ è½½: {self.model_path}")
            except Exception as e:
                logger.warning(f"[Error] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                need_train = True
        else:
            need_train = True

        if need_train:
            self.train()

    def _generate_enhanced_data(self):
        tool_samples = []
        chat_samples = []
        qa_samples = []  # [æ–°å¢] é—®ç­”æ ·æœ¬é›†åˆ

        entities = ["é›·ç¥", "èŒ…å°", "çº³æŒ‡", "ç‹è€…è£è€€", "åŸç¥", "è¿™åªè‚¡ç¥¨", "ä»Šå¤©", "Aè‚¡", "å²è±å§†", "é’Ÿç¦»", "ç«ç¥"]

        tool_patterns = [
            "å¸®æˆ‘ <ACT> <ENT>",
            "<ACT> æˆ‘çš„ <PROP>",
            "<ACT> <ENT> çš„ <PROP>",
            "<ACT> <ENT> <PROP>",
            "æ‰“å¼€ <ENT>",
            "<ACT> ä¸€å¼  <ENT>",
        ]

        chat_patterns = [
            "<NEG> <ACT>",
            "<NEG> <ACT> <ENT>",
            "<PROP> <STATE>",
            "<ENT> <STATE>",
            "<ENT> <NEG> <STATE>",
            "<STATE>",
            "æˆ‘ <NEG> çŸ¥é“",
            "<ACT> <NEG> <STATE>",
            "ä¸ºä»€ä¹ˆ <STATE>",
        ]

        # [æ–°å¢] é—®ç­”ä¸“ç”¨çš„å¥å¼ç»“æ„
        qa_patterns = [
            "<ENT> çš„ <KNOW> æ˜¯ <QUERY>",
            "<ENT> <KNOW> <QUERY>",
            "<QUERY> æ‰“ <ENT>",
            "<ENT> <KNOW> æ¨è",
            "æŸ¥ä¸€ä¸‹ <ENT> çš„ <KNOW>",
            "<ENT> åœ¨ <QUERY>",
            "<ENT> çš„ <KNOW> ä»‹ç»",
            "<ENT> <KNOW> <QUERY> æ­é…",
            "<KNOW> <QUERY> è·å¾—",
            "<ENT> çš„ <PROP> æ˜¯ <QUERY>",  # æœ‰äº›å±æ€§ä¹Ÿåå‘é—®ç­”ï¼Œå¦‚: é›·ç¥çš„é¢æ¿æ˜¯å¤šå°‘
        ]

        # ç”Ÿæˆå·¥å…·æ•°æ®
        for pattern in tool_patterns:
            for ent in entities:
                text = pattern.replace("<ENT>", ent)
                text = text.replace("<ACT>", random.choice(list(ACTION_VERBS)))
                text = text.replace("<PROP>", random.choice(list(FUNCTIONAL_NOUNS)))
                tool_samples.append(text.replace(" ", ""))

        # ç”Ÿæˆé—²èŠæ•°æ®
        for pattern in chat_patterns:
            for ent in entities:
                text = pattern.replace("<ENT>", ent)
                text = text.replace("<ACT>", random.choice(list(ACTION_VERBS)))
                text = text.replace("<PROP>", random.choice(list(FUNCTIONAL_NOUNS)))
                text = text.replace("<STATE>", random.choice(list(STATE_WORDS)))
                text = text.replace("<NEG>", random.choice(list(NEGATION_WORDS)))
                chat_samples.append(text.replace(" ", ""))

        # [æ–°å¢] ç”Ÿæˆé—®ç­”æ•°æ®
        for pattern in qa_patterns:
            for ent in entities:
                text = pattern.replace("<ENT>", ent)
                text = text.replace("<KNOW>", random.choice(list(KNOWLEDGE_NOUNS)))
                text = text.replace("<PROP>", random.choice(list(FUNCTIONAL_NOUNS)))
                text = text.replace("<QUERY>", random.choice(list(QUERY_WORDS)))
                qa_samples.append(text.replace(" ", ""))

        extra_chats = [
            "è¿™æ•°æ®å¤ªçœŸå®äº†",
            "å±æ€§æ‹‰èƒ¯",
            "çœ‹ä¸æ‡‚è¿™ä¸ªèµ°åŠ¿",
            "è¿™æ˜¯ä»€ä¹ˆé¬¼æ”»ç•¥",
            "åˆ«ç»™æˆ‘çœ‹è¿™äº›",
            "ä¸è¦åˆ†æ",
            "æˆ‘ä¸æŸ¥",
            "ç®—äº†å§",
            "ä½ æ˜¯è°",
            "ä½ å¥½",
        ]

        extra_qa = [
            "é›·ç¥çš„è¡€é‡æ˜¯å¤šå°‘",
            "è‰ç¥æ€ä¹ˆé…é˜Ÿ",
            "å²è±å§†åœ¨å“ªæŠ“",
            "é’Ÿç¦»çš„æŠ¤ç›¾æœºåˆ¶æ˜¯ä»€ä¹ˆ",
            "åŸç¥çš„èƒŒæ™¯æ•…äº‹æ˜¯ä»€ä¹ˆ",
            "è¿™ä¸ªä»»åŠ¡æ€ä¹ˆåš",
            "è¿™æŠŠæ­¦å™¨é€‚åˆè°",
            "å¤©èµ‹æ€ä¹ˆç‚¹",
        ]

        chat_samples.extend(extra_chats * 5)
        qa_samples.extend(extra_qa * 5)

        # ä¿è¯ä¸‰ç±»æ ·æœ¬æ•°é‡å‡è¡¡
        min_len = min(len(tool_samples), len(chat_samples), len(qa_samples))

        X = tool_samples[:min_len] + chat_samples[:min_len] + qa_samples[:min_len]
        y = ["å·¥å…·"] * min_len + ["é—²èŠ"] * min_len + ["é—®ç­”"] * min_len
        return X, y

    def train(self):
        logger.debug("[Info] å¼€å§‹è®­ç»ƒæ¨¡å‹(åŒ…å«å·¥å…·ã€é—²èŠã€é—®ç­”ä¸‰åˆ†ç±»)...")
        X_raw, y = self._generate_enhanced_data()
        X_abstract = [smart_abstraction(text) for text in X_raw]
        X_train_dict = {"raw": X_raw, "abs": X_abstract}

        pipeline = Pipeline(
            [
                (
                    "union",
                    FeatureUnion(
                        transformer_list=[
                            (
                                "abs_features",
                                Pipeline(
                                    [
                                        ("selector", ItemSelector(key="abs")),
                                        (
                                            "tfidf",
                                            TfidfVectorizer(token_pattern=r"(?u)\b\w+\b|<\w+>", ngram_range=(1, 3)),
                                        ),
                                    ]
                                ),
                            ),
                            (
                                "raw_features",
                                Pipeline(
                                    [
                                        ("selector", ItemSelector(key="raw")),
                                        (
                                            "tfidf",
                                            TfidfVectorizer(analyzer="char_wb", ngram_range=(2, 4), max_features=5000),
                                        ),
                                    ]
                                ),
                            ),
                        ]
                    ),
                ),
                ("clf", LogisticRegression(C=1.0, solver="lbfgs", class_weight="balanced")),
            ]
        )

        pipeline.fit(X_train_dict, y)
        dump(pipeline, self.model_path)
        self.model = pipeline
        logger.debug(f"[Info] æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶ä¿å­˜è‡³: {self.model_path}")

    def _rule_based_check(self, text: str) -> Optional[Dict[str, Any]]:
        # è§„åˆ™ 0: è‡ªèº«é—®é¢˜
        if re.search(
            r"^(æˆ‘|ä½ ).*(æ˜¯|ä½¿ç”¨|èƒ½|ä¼š).*(ä»€ä¹ˆ|è°|å•¥|æ€ä¹ˆ|å¤šå°‘|å¤šå¤§|åå­—|å‹å·).*(æ¨¡å‹|AI|åŠ©æ‰‹|æœºå™¨äºº|ç‰ˆæœ¬)",
            text,
        ):
            return {"intent": "é—²èŠ", "conf": 0.99, "reason": "Rule: SelfReference"}

        # è§„åˆ™ 1: [å·²ä¿®æ”¹] é˜²æ­¢è¯¯ä¼¤â€œå¥¹ç”¨ä»€ä¹ˆæ­¦å™¨(é—®ç­”)â€ã€‚ç°åœ¨åªåŒ¹é…çº¯ç²¹çš„â€œè¿™æ˜¯ä»€ä¹ˆâ€ç­‰æçŸ­å¥
        if re.search(r"^(è¿™|é‚£|æˆ‘|ä½ |ä»–|å¥¹|å®ƒ|å“ª|è°)[æ˜¯å«åšç©]?(ä»€ä¹ˆ|å’‹|è°|å“ª|å—|å‘¢)[?ï¼Ÿ]?$", text):
            return {"intent": "é—²èŠ", "conf": 0.98, "reason": "Rule: Pronoun+Query"}

        # è§„åˆ™ 2: çº¯ç–‘é—®/æƒ…ç»ªè¡¨è¾¾
        if re.search(r"^(ä¸ºä»€ä¹ˆ|å’‹å›äº‹|å•Š|å“å‘€|å‘œå‘œ|å“¼|å‘µå‘µ|å“ˆå“ˆ|å“‡|å”‰|å“å“Ÿ)+[!?ğŸ˜­ğŸ˜­ğŸ˜¢ğŸ˜±ğŸ˜¡ğŸ™]+.*$", text):
            return {"intent": "é—²èŠ", "conf": 0.95, "reason": "Rule: PureEmotion"}

        # è§„åˆ™ 3: è¯¢é—®è§‚ç‚¹/èº«ä»½/æ¨¡æ‹Ÿ
        if re.search(r".*(ä½ å¯¹.*çœ‹æ³•|ä½ è§‰å¾—|ä½ è®¤ä¸º|æ¨¡æ‹Ÿ|æ˜¯.*åŒ–èº«|ä½ åº”è¯¥|ä½ è¦|ä½ æ¯).*", text):
            return {"intent": "é—²èŠ", "conf": 0.93, "reason": "Rule: OpinionOrSimulate"}

        # è§„åˆ™ 4: åŠ¨è¯+å¦å®š/çŠ¶æ€
        if re.search(r"(æŸ¥|çœ‹|æœ|æ‰¾|åˆ†æ|ç®—|å¬|è¯´)(ä¸|æ²¡|æ— æ³•|ä¸èƒ½)(æ‡‚|äº†|åˆ°|è¡Œ|å¥½|æ˜ç™½)", text):
            return {"intent": "é—²èŠ", "conf": 0.97, "reason": "Rule: Act+Neg+State"}

        # è§„åˆ™ 5: å¼ºå¦å®š + åŠ¨ä½œ
        if re.search(r"[ä¸åˆ«æ²¡é][è¦]?.*?(æŸ¥|çœ‹|æœ|åˆ†æ|ç®—|æµ‹)", text):
            return {"intent": "é—²èŠ", "conf": 0.99, "reason": "Rule: Negation+Action"}

        # è§„åˆ™ 6: [æ–°å¢] å¼º RAG é—®ç­”ç‰¹å¾ (ç›´æ¥ç§’åˆ¤)
        if re.search(r".*(æ€ä¹ˆé…é˜Ÿ|è¡€é‡æ˜¯å¤šå°‘|åœ¨å“ªé‡Œ|æ€ä¹ˆæ‰“|èƒŒæ™¯æ•…äº‹|ä¸–ç•Œè§‚|æœºåˆ¶æ˜¯ä»€ä¹ˆ|æ¨è.+æ­¦å™¨).*", text):
            return {"intent": "é—®ç­”", "conf": 0.95, "reason": "Rule: StrongRAG"}

        return None

    def _sync_predict(self, text: str) -> Dict[str, Any]:
        rule_result = self._rule_based_check(text)
        if rule_result:
            return {"text": text, **rule_result}

        if self.model is None:
            return {"text": text, "intent": "Error", "conf": 0.0, "reason": "Model Not Loaded"}

        abstracted = smart_abstraction(text)
        input_data = {"raw": [text], "abs": [abstracted]}

        try:
            probs = self.model.predict_proba(input_data)[0]
            intent_idx = probs.argmax()
            intent = self.model.classes_[intent_idx]
            confidence = float(probs[intent_idx])
            return {"text": text, "intent": intent, "conf": round(confidence, 4), "reason": "Model"}
        except Exception as e:
            return {"text": text, "intent": "Error", "conf": 0.0, "reason": str(e)}

    async def predict_async(self, text: str) -> Dict[str, Any]:
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(self.executor, self._sync_predict, text)


# ==========================================
# 4. æµ‹è¯•ä¸è¿è¡Œ
# ==========================================


async def benchmark(service: IntentService):
    test_cases = [
        "å¸®æˆ‘ç”»ä¸€å¼ åŸç¥çš„å›¾ç‰‡",  # å·¥å…·
        "æŸ¥é¢æ¿",  # å·¥å…·
        "çœ‹çœ‹è‹±ä¼Ÿè¾¾èµ°åŠ¿",  # å·¥å…·
        "æ‰“å¼€ç©ºè°ƒ",  # å·¥å…·
        "å¸®æˆ‘çœ‹çœ‹æ·±æ¸Šè®°å½•",  # å·¥å…·
        "ç«ç¥é¢æ¿æ€ä¹ˆæå‡",  # é—®ç­”/å·¥å…· (çœ‹æ¨¡å‹æ€ä¹ˆåˆ†, åå‘é—®ç­”)
        "é›·ç¥æ€ä¹ˆé…é˜Ÿ",  # é—®ç­”
        "ç«å²è±å§†çš„è¡€é‡æ˜¯å¤šå°‘",  # é—®ç­”
        "åŸç¥çš„ä¸–ç•Œè§‚æ˜¯ä»€ä¹ˆ",  # é—®ç­”
        "è¿™æŠŠæ­¦å™¨é€‚åˆè°",  # é—®ç­”
        "é’Ÿç¦»çš„æŠ¤ç›¾æœºåˆ¶æ˜¯å•¥",  # é—®ç­”
        "æ·±æ¸Šæ€ä¹ˆæ‰“",  # é—®ç­”
        "æ·±æ¸Šå¥½éš¾æ‰“",  # é—²èŠ
        "é¢æ¿å¤ªä¸‘äº†",  # é—²èŠ
        "è‚¡ç¥¨äºéº»äº†",  # é—²èŠ
        "å§æ§½æ€ä¹ˆå›äº‹",  # é—²èŠ
        "è¿™æ˜¯ä»€ä¹ˆ",  # é—²èŠ
        "ä¸è¦æŸ¥",  # é—²èŠ
        "ä½ æ˜¯ä½¿ç”¨ä»€ä¹ˆæ¨¡å‹ï¼Ÿ",  # é—²èŠ
        "ä½ å¯¹æŠ±æŠ±çš„çœ‹æ³•æ˜¯ï¼Ÿ",  # é—²èŠ
    ]

    logger.debug(f"{'Input':<25} | {'Intent':<10} | {'Conf':<5} | {'Reason'}")
    logger.debug("-" * 70)

    tasks = [service.predict_async(t) for t in test_cases]
    results = await asyncio.gather(*tasks)

    for res in results:
        logger.debug(f"{res['text']:<25} | {res['intent']:<10} | {res['conf']:<5} | {res.get('reason', '-')}")


classifier_service = IntentService(model_path=MODEL_PATH)

if __name__ == "__main__":
    asyncio.run(benchmark(classifier_service))
